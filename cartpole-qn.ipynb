{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Cartpole Balancing problem  in OpenAI Env using Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "actions=range(env.action_space.n)\n",
    "\n",
    "alpha=0.01\n",
    "gamma=0.90\n",
    "epsilon=1\n",
    "\n",
    "episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_states_as_string():\n",
    "    states = []\n",
    "    for i in range(10000):\n",
    "        states.append(str(i).zfill(4))\n",
    "    return states\n",
    "\n",
    "# creates a dictionary that stores q-values for all states and actions corresponding to them\n",
    "def initialize():\n",
    "    q = {}\n",
    "    all_states = get_all_states_as_string()\n",
    "    for state in all_states:\n",
    "        q[state] = {}\n",
    "        for action in range(env.action_space.n):\n",
    "            q[state][action] = 0\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bins for digitization\n",
    "bins = np.zeros((4,10))\n",
    "bins[0] = np.linspace(-4.8, 4.8, 10)\n",
    "bins[1] = np.linspace(-5, 5, 10)\n",
    "bins[2] = np.linspace(-0.418, 0.418, 10)\n",
    "bins[3] = np.linspace(-5, 5, 10)\n",
    "\n",
    "def digitize(obs):\n",
    "    state = np.zeros(4)\n",
    "    for i in range(4):\n",
    "        state[i] = np.digitize(obs[i], bins[i])\n",
    "    return state\n",
    "\n",
    "# converting state of format = [float]*4 to a string \n",
    "def doHash(arr):\n",
    "    return ''.join(str(int(e)) for e in arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d):\n",
    "    max_v = float('-inf')\n",
    "    for key, val in d.items():\n",
    "        if val > max_v:\n",
    "            max_v = val\n",
    "            max_key = key\n",
    "    return max_key, max_v\n",
    "\n",
    "# updating q-value after each iteration\n",
    "def learn(state, action1, reward, new_state,q):\n",
    "    maxqnew = max_dict(q[new_state])\n",
    "    q[state][action1] += alpha*(reward + gamma*maxqnew[1] - q[state][action1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(epsilon):    \n",
    "    # new q dict\n",
    "    q = initialize()\n",
    "    for i_episode in range(episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        state = digitize(state)\n",
    "        state = doHash(state)\n",
    "        epsilon *= 0.99\n",
    "        cumulated_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if np.random.uniform() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = max_dict(q[state])[0]\n",
    "            \n",
    "            next_st, reward, done, info = env.step(action)\n",
    "            next_st = digitize(next_st)\n",
    "            next_st = doHash(next_st)\n",
    "            cumulated_reward += reward\n",
    "\n",
    "            if done:\n",
    "                reward = -300\n",
    "            learn(state, action, reward, next_st,q)\n",
    "            state = next_st\n",
    "            \n",
    "        if i_episode%100==0:\n",
    "            print(\"Episode = \" +str(i_episode) + \", Score = \" + str(cumulated_reward))\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode = 0, Score = 12.0\n",
      "Episode = 100, Score = 86.0\n",
      "Episode = 200, Score = 96.0\n",
      "Episode = 300, Score = 136.0\n",
      "Episode = 400, Score = 95.0\n",
      "Episode = 500, Score = 146.0\n",
      "Episode = 600, Score = 131.0\n",
      "Episode = 700, Score = 130.0\n",
      "Episode = 800, Score = 151.0\n",
      "Episode = 900, Score = 125.0\n",
      "Episode = 1000, Score = 148.0\n",
      "Episode = 1100, Score = 162.0\n",
      "Episode = 1200, Score = 134.0\n",
      "Episode = 1300, Score = 144.0\n",
      "Episode = 1400, Score = 159.0\n",
      "Episode = 1500, Score = 142.0\n",
      "Episode = 1600, Score = 147.0\n",
      "Episode = 1700, Score = 146.0\n",
      "Episode = 1800, Score = 143.0\n",
      "Episode = 1900, Score = 146.0\n"
     ]
    }
   ],
   "source": [
    "Q = train(epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def play(Q):\n",
    "#     env.reset()\n",
    "#     for i in range(200):\n",
    "#         env.render()\n",
    "#         if i==0:\n",
    "#             action = env.action_space.sample()\n",
    "#         else:\n",
    "#             action = max_dict(q[state])[0]\n",
    "#         next_st, reward, done, info = env.step(action)\n",
    "#         print(next_st)\n",
    "#         state = doHash(digitize(next_st))\n",
    "#         if done:\n",
    "#             break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
