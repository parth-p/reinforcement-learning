{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Cartpole Balancing problem/Atari Problemms in OpenAI Env using Dueling DQN network along with prioritized memory replay using sum tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize\n",
    "import math, random\n",
    "import gym\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.autograd as Variable\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilonMin = 0.01\n",
    "decay = 0.999\n",
    "episodes = 500\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "goal_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, 128)\n",
    "        \n",
    "        self.a1 = nn.Linear(128, 128)\n",
    "        self.a2 = nn.Linear(128, num_outputs)\n",
    "        \n",
    "        self.val1 = nn.Linear(128, 128)\n",
    "        self.val2 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        adv = F.relu(self.a1(x))\n",
    "        adv = self.a2(adv)\n",
    "\n",
    "        val = F.relu(self.val1(x))\n",
    "        val = self.val2(val)\n",
    "        return val + adv - adv.mean()\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_value = self.forward(state)\n",
    "            action = q_value.max(1)[1].data[0].numpy()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = DuelingDQN(env.observation_space.shape[0], env.action_space.n)\n",
    "model2 = DuelingDQN(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "optimizer = optim.Adam(model1.parameters())\n",
    "def sync(model1, model2):\n",
    "    model2.load_state_dict(model1.state_dict())\n",
    "\n",
    "sync(model1, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    write = 0\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2*capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx-1)//2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2*idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replay memory\n",
    "class Memory:\n",
    "    samples = []\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.e = 0.01\n",
    "        self.a = 0.6\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.e)**self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        segment = self.tree.total()/n\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment*1\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a,b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            batch.append((idx, data))\n",
    "            \n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimating error\n",
    "def get_error(state, action, reward, next_state, done):\n",
    "    state = torch.FloatTensor(np.float32(state))\n",
    "    next_state = torch.FloatTensor(np.float32(next_state))\n",
    "    action = torch.LongTensor(action)\n",
    "#     print(action.unsqueeze(1))\n",
    "    reward = torch.FloatTensor(reward)\n",
    "    done = torch.FloatTensor(done)\n",
    "    q_values = model1(state)\n",
    "    next_q_values = model1(next_state)\n",
    "#     print(next_q_values.shape)\n",
    "    next_q_values2 = model2(next_state)\n",
    "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value = next_q_values2.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    error = abs(q_value - expected_q_value)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory = Memory(10000)\n",
    "\n",
    "for idx in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = int(model1.act(state, epsilon))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        exp = state, action, reward, next_state, done\n",
    "\n",
    "        state = torch.FloatTensor(np.float32(state))\n",
    "        next_state = torch.FloatTensor(np.float32(next_state))\n",
    "        q_values = model1(state)\n",
    "        next_q_values = model1(next_state)\n",
    "        next_q_values2 = model2(next_state)\n",
    "\n",
    "        q_value = q_values[action].squeeze(0)\n",
    "        a = int(torch.max(next_q_values, 0)[1].numpy())\n",
    "        next_q_value = next_q_values2[a]\n",
    "        \n",
    "        expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "        error = float(abs(q_value - expected_q_value).detach().numpy())\n",
    "        memory.add(error, exp)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            done = False\n",
    "            print(\"Episode = \" + str(idx) + \" , Score = \" + str(total_reward))\n",
    "            break\n",
    "    \n",
    "    if epsilon > epsilonMin:\n",
    "        epsilon *= decay\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "        sync(model1, model2)\n",
    "        \n",
    "    if idx > 3:\n",
    "        batch = memory.sample(batch_size)\n",
    "        state = [np.array(batch[i][1][0]) for i in range(batch_size)]\n",
    "        action = np.array([o[1][1] for o in batch])\n",
    "        reward = np.array([o[1][2] for o in batch])\n",
    "        next_state = np.array([o[1][3] for o in batch])\n",
    "        done = np.array([o[1][4] for o in batch])\n",
    "        d = [0]*32\n",
    "        for i in range(len(d)):\n",
    "            if done[i]==True:\n",
    "                d[i] = 1\n",
    "            else:\n",
    "                d[i] = 0\n",
    "        error = get_error(state, action, reward, next_state, d)\n",
    "        loss = error.pow(2).mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Atari Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess reduces dimension and converts frame of 210x160x3 to 84x84\n",
    "def preprocess(observation):\n",
    "    observation = cv2.cvtColor(cv2.resize(observation, (84, 110)), cv2.COLOR_BGR2GRAY)\n",
    "    observation = observation[26:110,:]\n",
    "    ret, observation = cv2.threshold(observation, 1, 255, cv2.THRESH_BINARY)\n",
    "    return np.reshape(observation, (84, 84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stack 4 frames (4 frames used to give idea of motion) to create data set\n",
    "stack_size = 4\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame = preprocess(state)\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        stacked_state = np.stack(stacked_frames, axis=0)\n",
    "        \n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=0)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Conv. neural network for training\n",
    "class DuelingCnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DuelingCnnDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.adv1 = nn.Linear(7*7*64, 512)\n",
    "        self.adv2 = nn.Linear(512, num_actions)\n",
    "        \n",
    "        self.val1 = nn.Linear(7*7*64, 512)\n",
    "        self.val2 = nn.Linear(512, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        advantage = F.relu(self.adv1(x))\n",
    "        advantage = self.adv2(advantage)\n",
    "        \n",
    "        value = F.relu(self.val1(x))\n",
    "        value = self.val2(value)\n",
    "        \n",
    "        return value + advantage  - advantage.mean()\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_value = self.forward(state)\n",
    "            action = q_value.max(1)[1].data[0].numpy()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "state = env.reset()\n",
    "state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "input_size = state.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DuelingCnnDQN(input_size,action_size)\n",
    "model2  = DuelingCnnDQN(input_size,action_size)\n",
    "\n",
    "def sync(model1, model2):\n",
    "    model2.load_state_dict(model1.state_dict())\n",
    "\n",
    "sync(model1, model2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
