{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import collections\n",
    "import copy\n",
    "import gym\n",
    "import numpy as np\n",
    "from operator import mul\n",
    "from functools import reduce\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.nn.utils.convert_parameters import vector_to_parameters, parameters_to_vector\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "episodes = 1000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "goal_steps = 200\n",
    "input_shape = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "# print(num_actions)\n",
    "buffer_capacity = 1000\n",
    "epochs = 5\n",
    "clip_param = 0.2\n",
    "tau = 0.97\n",
    "max_kl=0.001\n",
    "damping=0.001\n",
    "iters=10\n",
    "residual_tol=1e-10\n",
    "ent_coeff=0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Buffer(object):\n",
    "    def __init__(self):\n",
    "        self.buffer = []\n",
    "        self.buffer_capacity = 1000\n",
    "        self.batch = 32\n",
    "    \n",
    "    def add(self, params):\n",
    "        self.buffer.append(params)\n",
    "        \n",
    "    def reinit(self):\n",
    "        self.buffer = []\n",
    "        \n",
    "    def length(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.mu_head = nn.Linear(128,num_actions)\n",
    "        self.log_std = nn.Parameter(torch.zeros(num_actions))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.mu_head(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        dist = Normal(mu, std)\n",
    "        return dist\n",
    "\n",
    "class Value(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(Value, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        value = self.fc3(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,input_shape, num_actions):\n",
    "        self.device   = torch.device(\"cpu\")\n",
    "        self.policy = Policy(input_shape, num_actions)#.to(self.device)\n",
    "        self.value = Value(input_shape, num_actions)#.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.value.parameters())\n",
    "        self.buffer_capacity = 1000\n",
    "        self.batch_size = 32\n",
    "        self.policy_model_properties = collections.OrderedDict()\n",
    "        for k, v in self.policy.state_dict().items():\n",
    "            self.policy_model_properties[k] = v.size()\n",
    "\n",
    "    def kl_divergence(self, model):\n",
    "#         observations_tensor = torch.cat([Tensor(observation).unsqueeze(0) for observation in self.observations])\n",
    "        dist_n = model(self.s)\n",
    "        actprob = dist_n.log_prob(self.a.unsqueeze(1))\n",
    "        old_dist = self.policy(self.s)\n",
    "        old_actprob = old_dist.log_prob(self.a.unsqueeze(1))\n",
    "        return torch.sum(torch.exp(old_actprob) * (old_actprob / actprob)).mean()\n",
    "\n",
    "    def hessian_vector_product(self, vector):\n",
    "        self.policy.zero_grad()\n",
    "        kl_div = self.kl_divergence(self.policy)\n",
    "        kl_div.backward(create_graph=True)\n",
    "        gradient = flatten_model_params([p.grad for p in self.policy.parameters()]).squeeze(0)\n",
    "        gradient_vector_product = torch.sum(gradient * vector)\n",
    "        gradient_vector_product.backward()#torch.ones(gradient.size())\n",
    "        return (flatten_model_params([p.grad for p in self.policy.parameters()]).squeeze(0) - gradient).data \n",
    "\n",
    "    def conjugate_gradient(self, b):\n",
    "        p = b.clone().data\n",
    "        r = b.clone().data\n",
    "        x = np.zeros_like(b.data.numpy())\n",
    "        rdotr = r.dot(r)\n",
    "        for i in range(iters):\n",
    "            z = self.hessian_vector_product(p)\n",
    "            v = rdotr / p.dot(z)\n",
    "            x += v * p\n",
    "            r -= v * z\n",
    "            newrdotr = r.dot(r)\n",
    "            mu = newrdotr / rdotr\n",
    "            p = r + mu * p\n",
    "            rdotr = newrdotr\n",
    "            if rdotr < residual_tol:\n",
    "                break\n",
    "        return x\n",
    "    \n",
    "    def linesearch(self, x, fullstep, expected_improve_rate):\n",
    "        accept_ratio = .1\n",
    "        max_backtracks = 10\n",
    "        \n",
    "        fval = self.surrogate_loss(x)\n",
    "        for (_n_backtracks, stepfrac) in enumerate(.5**np.arange(max_backtracks)):\n",
    "            xnew = x.data.numpy() + stepfrac * fullstep\n",
    "            newfval = self.surrogate_loss(torch.from_numpy(xnew))\n",
    "            actual_improve = fval - newfval\n",
    "            expected_improve = expected_improve_rate * stepfrac\n",
    "            ratio = actual_improve / expected_improve\n",
    "            if ratio > accept_ratio and actual_improve > 0:\n",
    "                return torch.from_numpy(xnew)\n",
    "        return x\n",
    "    \n",
    "    def surrogate_loss(self, theta):\n",
    "        new_model = self.construct_model_from_theta(theta.data)\n",
    "#         observations_tensor = torch.cat([Tensor(observation).unsqueeze(0) for observation in self.observations])\n",
    "        dist_new = new_model(self.s)\n",
    "        prob_new = dist_new.log_prob(self.a.unsqueeze(1))\n",
    "        \n",
    "        dist_old = new_model(self.s)\n",
    "        prob_old = dist_old.log_prob(self.a.unsqueeze(1))\n",
    "        \n",
    "        return -torch.sum(torch.exp(prob_new / prob_old) * self.advantages)\n",
    "    \n",
    "    def construct_model_from_theta(self, theta):\n",
    "        theta = theta.squeeze(0)\n",
    "        new_model = copy.deepcopy(self.policy)\n",
    "        state_dict = collections.OrderedDict()\n",
    "        start_index = 0\n",
    "        for k, v in self.policy_model_properties.items():\n",
    "            param_length = reduce(mul, v, 1)\n",
    "            state_dict[k] = theta[start_index : start_index + param_length].view(v)\n",
    "            start_index += param_length\n",
    "        new_model.load_state_dict(state_dict)\n",
    "        return new_model\n",
    "\n",
    "    def update(self, entropy):\n",
    "        mem = memory.buffer\n",
    "        self.s = torch.FloatTensor([m[0].numpy() for m in mem])\n",
    "        self.a = torch.FloatTensor([m[1] for m in mem]) \n",
    "        old_log_a = torch.FloatTensor([m[2] for m in mem])\n",
    "        r = torch.FloatTensor([m[3] for m in mem])\n",
    "        masks = torch.FloatTensor([m[4] for m in mem])\n",
    "        values = self.value(self.s)\n",
    "#         print(a.size(),self.s.size())\n",
    "        returns = torch.Tensor(self.a.size(0),1)\n",
    "        deltas = torch.Tensor(self.a.size(0),1)\n",
    "        self.advantages = torch.Tensor(self.a.size(0),1)\n",
    "        prev_return = 0\n",
    "        prev_value = 0\n",
    "        prev_advantage = 0\n",
    "        for i in reversed(range(len(r))):\n",
    "            returns[i] = r[i] + gamma * prev_return * masks[i]\n",
    "            deltas[i] = r[i] + gamma * prev_value * masks[i] - values[i]\n",
    "            self.advantages[i] = deltas[i] + gamma * tau * prev_advantage * masks[i]\n",
    "            prev_return = returns[i, 0]\n",
    "            prev_value = values.data[i, 0]\n",
    "            prev_advantage = self.advantages[i, 0]\n",
    "        self.advantages = self.advantages.squeeze(1)\n",
    "#         print(advantages.size())\n",
    "        for _ in range(epochs):\n",
    "            for id in BatchSampler(SubsetRandomSampler(range(200)), batch_size, False):\n",
    "                dist = self.policy(self.s[id])\n",
    "                new_log_a = dist.log_prob(self.a[id].unsqueeze(1))\n",
    "                ratio = torch.exp(new_log_a.squeeze(1) - old_log_a[id])\n",
    "                surrogate_loss = -torch.mean(ratio * self.advantages[id]) - (ent_coeff * entropy)\n",
    "\n",
    "                self.policy.zero_grad()\n",
    "                surrogate_loss.backward(retain_graph=True)\n",
    "                policy_gradient = flatten_model_params([p.grad for p in self.policy.parameters()]).squeeze(0)\n",
    "                \n",
    "                step_direction = self.conjugate_gradient(policy_gradient)\n",
    "                step_direction_variable = torch.from_numpy(step_direction)#.unsqueeze(1)\n",
    "                \n",
    "                shs = .5 * step_direction.dot(self.hessian_vector_product(step_direction_variable).numpy().T)\n",
    "                lm = np.sqrt(shs / max_kl)\n",
    "                fullstep = step_direction / lm\n",
    "                gdotstepdir = policy_gradient.dot(step_direction_variable)#.data[0]\n",
    "#                 print(list(self.policy.parameters()))\n",
    "                theta = self.linesearch(flatten_model_params(list(self.policy.parameters())), fullstep, gdotstepdir / lm)\n",
    "\n",
    "                # Update parameters of policy model\n",
    "                old_model = copy.deepcopy(self.policy)\n",
    "                old_model.load_state_dict(self.policy.state_dict())\n",
    "                self.policy = self.construct_model_from_theta(theta.data)\n",
    "                kl_old_new = self.kl_divergence(old_model)\n",
    "\n",
    "                self.fit(returns)\n",
    "\n",
    "        memory.reinit()\n",
    "    \n",
    "    def fit(self, labels):\n",
    "        def closure():\n",
    "            predicted = self.value(self.s)\n",
    "            loss = torch.pow(predicted - labels, 2).sum()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        self.optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# policy = Policy(input_shape, num_actions)#.to(self.device)\n",
    "\n",
    "def flatten_model_params(parameters):\n",
    "    return torch.cat([param.view(1, -1) for param in parameters], 1)\n",
    "\n",
    "# grad = list(policy.parameters())\n",
    "# x = flatten_model_params(grad)\n",
    "# print(x)\n",
    "agent = Agent(input_shape, num_actions)\n",
    "# value = Value(input_shape, num_actions)#.to(self.device)\n",
    "# optimizer = optim.Adam(value.parameters())\n",
    "\n",
    "# policy_model_properties = collections.OrderedDict()\n",
    "# for k, v in policy.state_dict().items():\n",
    "#     policy_model_properties[k] = v.size()\n",
    "# print(policy_model_properties)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parth/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:125: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode = 0, Score = -1143.93262294\n",
      "Episode = 1, Score = -874.171427413\n",
      "Episode = 2, Score = -1322.27626545\n",
      "Episode = 3, Score = -1475.88116508\n",
      "Episode = 4, Score = -1247.32785124\n",
      "Episode = 5, Score = -1667.22229589\n",
      "Episode = 6, Score = -1254.95440065\n",
      "Episode = 7, Score = -1268.68848168\n",
      "Episode = 8, Score = -1275.98459647\n",
      "Episode = 9, Score = -916.64800343\n",
      "Episode = 10, Score = -1372.89634395\n",
      "Episode = 11, Score = -1218.54830486\n",
      "Episode = 12, Score = -1177.36439435\n",
      "Episode = 13, Score = -1576.38362215\n",
      "Episode = 14, Score = -1141.50814625\n",
      "Episode = 15, Score = -1363.90907364\n",
      "Episode = 16, Score = -1449.83774515\n",
      "Episode = 17, Score = -1040.214042\n",
      "Episode = 18, Score = -1694.99835944\n",
      "Episode = 19, Score = -1437.34522919\n",
      "Episode = 20, Score = -1292.74964927\n",
      "Episode = 21, Score = -964.911276204\n",
      "Episode = 22, Score = -1504.7347425\n",
      "Episode = 23, Score = -829.341913425\n",
      "Episode = 24, Score = -1602.93754158\n",
      "Episode = 25, Score = -963.683507892\n",
      "Episode = 26, Score = -1081.94721951\n",
      "Episode = 27, Score = -1599.11952091\n",
      "Episode = 28, Score = -1625.11839478\n",
      "Episode = 29, Score = -1657.56705073\n",
      "Episode = 30, Score = -1771.1739714\n",
      "Episode = 31, Score = -1121.63138472\n",
      "Episode = 32, Score = -1098.23731868\n",
      "Episode = 33, Score = -1069.9553358\n",
      "Episode = 34, Score = -1545.83780486\n",
      "Episode = 35, Score = -750.553058542\n",
      "Episode = 36, Score = -1171.51418555\n",
      "Episode = 37, Score = -1568.54953641\n",
      "Episode = 38, Score = -742.038952011\n",
      "Episode = 39, Score = -1541.50372061\n",
      "Episode = 40, Score = -898.697858725\n",
      "Episode = 41, Score = -1353.43356879\n",
      "Episode = 42, Score = -1261.83661747\n",
      "Episode = 43, Score = -1436.89132634\n",
      "Episode = 44, Score = -1724.04357145\n",
      "Episode = 45, Score = -973.153839106\n",
      "Episode = 46, Score = -1354.71771755\n",
      "Episode = 47, Score = -928.344676535\n",
      "Episode = 48, Score = -1044.33162406\n",
      "Episode = 49, Score = -1252.7402129\n",
      "Episode = 50, Score = -1202.67787566\n",
      "Episode = 51, Score = -1719.3263351\n",
      "Episode = 52, Score = -1642.24006568\n",
      "Episode = 53, Score = -1797.16270442\n",
      "Episode = 54, Score = -1370.71511336\n",
      "Episode = 55, Score = -1080.72317021\n",
      "Episode = 56, Score = -1538.10537424\n",
      "Episode = 57, Score = -884.759288752\n",
      "Episode = 58, Score = -1799.7344092\n",
      "Episode = 59, Score = -890.221315274\n",
      "Episode = 60, Score = -1085.80882124\n",
      "Episode = 61, Score = -1180.97811983\n",
      "Episode = 62, Score = -1790.04250026\n",
      "Episode = 63, Score = -1621.22702851\n",
      "Episode = 64, Score = -1329.38380665\n",
      "Episode = 65, Score = -1538.68720901\n",
      "Episode = 66, Score = -1156.70179022\n",
      "Episode = 67, Score = -980.868227754\n",
      "Episode = 68, Score = -962.559893592\n",
      "Episode = 69, Score = -1183.95919158\n",
      "Episode = 70, Score = -1372.10386699\n",
      "Episode = 71, Score = -1386.03139597\n",
      "Episode = 72, Score = -871.282789704\n",
      "Episode = 73, Score = -1445.88020018\n",
      "Episode = 74, Score = -1290.39836653\n",
      "Episode = 75, Score = -1652.93011794\n",
      "Episode = 76, Score = -1165.54853103\n",
      "Episode = 77, Score = -1050.27637623\n",
      "Episode = 78, Score = -1261.90143216\n",
      "Episode = 79, Score = -973.933358166\n",
      "Episode = 80, Score = -1006.24275387\n",
      "Episode = 81, Score = -1296.41788951\n",
      "Episode = 82, Score = -1262.788646\n",
      "Episode = 83, Score = -1425.71294811\n",
      "Episode = 84, Score = -1340.30133389\n",
      "Episode = 85, Score = -968.504352854\n",
      "Episode = 86, Score = -1499.85171436\n",
      "Episode = 87, Score = -1399.3120932\n",
      "Episode = 88, Score = -1306.41471046\n",
      "Episode = 89, Score = -966.118172385\n",
      "Episode = 90, Score = -1687.83888556\n",
      "Episode = 91, Score = -1746.32161081\n",
      "Episode = 92, Score = -972.890563191\n",
      "Episode = 93, Score = -755.015903241\n",
      "Episode = 94, Score = -1306.51626225\n",
      "Episode = 95, Score = -1168.57686057\n",
      "Episode = 96, Score = -1070.17003284\n",
      "Episode = 97, Score = -1463.90165304\n",
      "Episode = 98, Score = -1750.7999904\n",
      "Episode = 99, Score = -1748.80793365\n",
      "Episode = 100, Score = -1004.66196136\n",
      "Episode = 101, Score = -1129.47748343\n",
      "Episode = 102, Score = -1666.76857362\n",
      "Episode = 103, Score = -1080.73607398\n",
      "Episode = 104, Score = -963.968056544\n",
      "Episode = 105, Score = -856.58682805\n",
      "Episode = 106, Score = -1000.5322749\n",
      "Episode = 107, Score = -1218.85771554\n",
      "Episode = 108, Score = -1324.91489143\n",
      "Episode = 109, Score = -1509.30232037\n",
      "Episode = 110, Score = -1322.15609366\n",
      "Episode = 111, Score = -1598.66138323\n",
      "Episode = 112, Score = -1853.64626247\n",
      "Episode = 113, Score = -1434.8660255\n",
      "Episode = 114, Score = -1017.40818508\n",
      "Episode = 115, Score = -1483.03908521\n",
      "Episode = 116, Score = -1166.08715591\n",
      "Episode = 117, Score = -1185.0846463\n",
      "Episode = 118, Score = -1150.16935731\n",
      "Episode = 119, Score = -907.25096403\n",
      "Episode = 120, Score = -1240.34023589\n",
      "Episode = 121, Score = -1383.89083791\n",
      "Episode = 122, Score = -1163.18258558\n",
      "Episode = 123, Score = -1168.86834552\n",
      "Episode = 124, Score = -772.069847804\n",
      "Episode = 125, Score = -1200.68879274\n",
      "Episode = 126, Score = -1470.7880058\n",
      "Episode = 127, Score = -816.320489764\n",
      "Episode = 128, Score = -1068.01256797\n",
      "Episode = 129, Score = -978.170407036\n",
      "Episode = 130, Score = -1483.76184292\n",
      "Episode = 131, Score = -1376.65154688\n",
      "Episode = 132, Score = -1431.39578251\n",
      "Episode = 133, Score = -857.834229406\n",
      "Episode = 134, Score = -1366.65259672\n",
      "Episode = 135, Score = -1179.54174545\n",
      "Episode = 136, Score = -874.857758095\n",
      "Episode = 137, Score = -1482.78450293\n",
      "Episode = 138, Score = -866.255546231\n",
      "Episode = 139, Score = -865.905930375\n",
      "Episode = 140, Score = -1174.97607183\n",
      "Episode = 141, Score = -1178.63710634\n",
      "Episode = 142, Score = -1183.75744397\n",
      "Episode = 143, Score = -1739.146346\n",
      "Episode = 144, Score = -1645.37474466\n",
      "Episode = 145, Score = -1262.83875933\n",
      "Episode = 146, Score = -1131.42931859\n",
      "Episode = 147, Score = -1078.01360346\n",
      "Episode = 148, Score = -1015.327155\n",
      "Episode = 149, Score = -1024.67942769\n",
      "Episode = 150, Score = -1031.79071516\n",
      "Episode = 151, Score = -1396.10397237\n",
      "Episode = 152, Score = -1136.76979713\n",
      "Episode = 153, Score = -1656.33295402\n",
      "Episode = 154, Score = -1005.28845789\n",
      "Episode = 155, Score = -970.442888816\n",
      "Episode = 156, Score = -1287.4184492\n",
      "Episode = 157, Score = -967.822756301\n",
      "Episode = 158, Score = -1309.29059097\n",
      "Episode = 159, Score = -856.028841128\n",
      "Episode = 160, Score = -1374.37256158\n",
      "Episode = 161, Score = -1382.26419889\n",
      "Episode = 162, Score = -972.915714412\n",
      "Episode = 163, Score = -1384.17109378\n",
      "Episode = 164, Score = -1186.17457491\n",
      "Episode = 165, Score = -1185.27850765\n",
      "Episode = 166, Score = -881.785883236\n",
      "Episode = 167, Score = -1586.29881936\n",
      "Episode = 168, Score = -1144.71484497\n",
      "Episode = 169, Score = -882.171067704\n",
      "Episode = 170, Score = -1653.4582597\n",
      "Episode = 171, Score = -947.76233069\n",
      "Episode = 172, Score = -877.726922327\n",
      "Episode = 173, Score = -1073.77006481\n",
      "Episode = 174, Score = -1000.35604342\n",
      "Episode = 175, Score = -922.981844916\n",
      "Episode = 176, Score = -1477.30212304\n",
      "Episode = 177, Score = -1174.48034971\n",
      "Episode = 178, Score = -1258.97034948\n",
      "Episode = 179, Score = -1467.35415126\n",
      "Episode = 180, Score = -871.18714062\n",
      "Episode = 181, Score = -889.544044274\n",
      "Episode = 182, Score = -1446.70930111\n",
      "Episode = 183, Score = -1177.21551613\n",
      "Episode = 184, Score = -1198.75286714\n",
      "Episode = 185, Score = -1070.86835584\n",
      "Episode = 186, Score = -1169.36563081\n",
      "Episode = 187, Score = -1507.67863531\n",
      "Episode = 188, Score = -1688.44889023\n",
      "Episode = 189, Score = -984.220320358\n",
      "Episode = 190, Score = -875.412820431\n",
      "Episode = 191, Score = -993.954115322\n",
      "Episode = 192, Score = -1188.47074231\n",
      "Episode = 193, Score = -1356.04034595\n",
      "Episode = 194, Score = -1084.95536873\n",
      "Episode = 195, Score = -1660.75318699\n",
      "Episode = 196, Score = -1601.05242011\n",
      "Episode = 197, Score = -1167.03003599\n",
      "Episode = 198, Score = -1071.3074691\n",
      "Episode = 199, Score = -1338.55717419\n",
      "Episode = 200, Score = -1019.60398436\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-f492515b6e8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", Score = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-240-3c4d0769f67e>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, entropy)\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mpolicy_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_model_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mstep_direction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconjugate_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m                 \u001b[0mstep_direction_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_direction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.unsqueeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-240-3c4d0769f67e>\u001b[0m in \u001b[0;36mconjugate_gradient\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mrdotr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhessian_vector_product\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdotr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-240-3c4d0769f67e>\u001b[0m in \u001b[0;36mhessian_vector_product\u001b[0;34m(self, vector)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mkl_div\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mkl_div\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_model_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mgradient_vector_product\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "memory = Buffer()\n",
    "state = env.reset()\n",
    "for idx in range(episodes):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    entropy = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state)\n",
    "        dist = agent.policy(state)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "#         action = action.clamp(-2, 2)\n",
    "#         print(action)\n",
    "        next_state, reward, done, _ = env.step(action.numpy())\n",
    "        score += reward\n",
    "        memory.add([state, action, log_prob, reward, 1 - done])\n",
    "        state = next_state\n",
    "        \n",
    "    agent.update(entropy)\n",
    "    memory.reinit()\n",
    "    print(\"Episode = \" + str(idx) + \", Score = \" + str(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
